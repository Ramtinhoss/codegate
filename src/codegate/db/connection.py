import asyncio
import hashlib
import json
import re
from datetime import timedelta
from pathlib import Path
from typing import List, Optional

import structlog
from pydantic import BaseModel
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine

from codegate.config import Config
from codegate.db.models import Alert, Output, Prompt
from codegate.db.queries import (
    AsyncQuerier,
    GetAlertsWithPromptAndOutputRow,
    GetPromptWithOutputsRow,
)
from codegate.pipeline.base import PipelineContext

logger = structlog.get_logger("codegate")
alert_queue = asyncio.Queue()
fim_entries = {}


class DbCodeGate:

    def __init__(self, sqlite_path: Optional[str] = None):
        # Initialize SQLite database engine with proper async URL
        if not sqlite_path:
            current_dir = Path(__file__).parent
            sqlite_path = (
                current_dir.parent.parent.parent / "codegate_volume" / "db" / "codegate.db"
            )
        self._db_path = Path(sqlite_path).absolute()
        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        logger.debug(f"Initializing DB from path: {self._db_path}")
        engine_dict = {
            "url": f"sqlite+aiosqlite:///{self._db_path}",
            "echo": False,  # Set to False in production
            "isolation_level": "AUTOCOMMIT",  # Required for SQLite
        }
        self._async_db_engine = create_async_engine(**engine_dict)

    def does_db_exist(self):
        return self._db_path.is_file()


class DbRecorder(DbCodeGate):

    def __init__(self, sqlite_path: Optional[str] = None):
        super().__init__(sqlite_path)

        if not self.does_db_exist():
            logger.info(f"Database does not exist at {self._db_path}. Creating..")
            asyncio.run(self.init_db())

    async def init_db(self):
        """Initialize the database with the schema."""
        if self.does_db_exist():
            logger.info("Database already exists. Skipping initialization.")
            return

        # Get the absolute path to the schema file
        current_dir = Path(__file__).parent
        schema_path = current_dir.parent.parent.parent / "sql" / "schema" / "schema.sql"

        if not schema_path.exists():
            raise FileNotFoundError(f"Schema file not found at {schema_path}")

        # Read the schema
        with open(schema_path, "r") as f:
            schema = f.read()

        try:
            # Execute the schema
            async with self._async_db_engine.begin() as conn:
                # Split the schema into individual statements and execute each one
                statements = [stmt.strip() for stmt in schema.split(";") if stmt.strip()]
                for statement in statements:
                    # Use SQLAlchemy text() to create executable SQL statements
                    await conn.execute(text(statement))
        finally:
            await self._async_db_engine.dispose()

    async def _insert_pydantic_model(
        self, model: BaseModel, sql_insert: text
    ) -> Optional[BaseModel]:
        # There are create method in queries.py automatically generated by sqlc
        # However, the methods are buggy for Pydancti and don't work as expected.
        # Manually writing the SQL query to insert Pydantic models.
        async with self._async_db_engine.begin() as conn:
            try:
                result = await conn.execute(sql_insert, model.model_dump())
                row = result.first()
                if row is None:
                    return None

                # Get the class of the Pydantic object to create a new object
                model_class = model.__class__
                return model_class(**row._asdict())
            except Exception as e:
                logger.error(f"Failed to insert model: {model}.", error=str(e))
                return None

    async def record_request(self, prompt_params: Optional[Prompt] = None) -> Optional[Prompt]:
        if prompt_params is None:
            return None
        sql = text(
            """
                INSERT INTO prompts (id, timestamp, provider, request, type)
                VALUES (:id, :timestamp, :provider, :request, :type)
                RETURNING *
                """
        )
        recorded_request = await self._insert_pydantic_model(prompt_params, sql)
        logger.debug(f"Recorded request: {recorded_request}")
        return recorded_request

    async def record_outputs(self, outputs: List[Output]) -> Optional[Output]:
        if not outputs:
            return

        first_output = outputs[0]
        # Create a single entry on DB but encode all of the chunks in the stream as a list
        # of JSON objects in the field `output`
        output_db = Output(
            id=first_output.id,
            prompt_id=first_output.prompt_id,
            timestamp=first_output.timestamp,
            output=first_output.output,
        )
        full_outputs = []
        # Just store the model respnses in the list of JSON objects.
        for output in outputs:
            full_outputs.append(output.output)
        output_db.output = json.dumps(full_outputs)

        sql = text(
            """
                INSERT INTO outputs (id, prompt_id, timestamp, output)
                VALUES (:id, :prompt_id, :timestamp, :output)
                RETURNING *
                """
        )
        recorded_output = await self._insert_pydantic_model(output_db, sql)
        logger.debug(f"Recorded output: {recorded_output}")
        return recorded_output

    async def record_alerts(self, alerts: List[Alert]) -> List[Alert]:
        if not alerts:
            return
        sql = text(
            """
                INSERT INTO alerts (
                id, prompt_id, code_snippet, trigger_string, trigger_type, trigger_category,
                timestamp
                )
                VALUES (:id, :prompt_id, :code_snippet, :trigger_string, :trigger_type,
                :trigger_category, :timestamp)
                RETURNING *
                """
        )
        # We can insert each alert independently in parallel.
        alerts_tasks = []
        async with asyncio.TaskGroup() as tg:
            for alert in alerts:
                try:
                    result = tg.create_task(self._insert_pydantic_model(alert, sql))
                    alerts_tasks.append(result)
                except Exception as e:
                    logger.error(f"Failed to record alert: {alert}.", error=str(e))

        recorded_alerts = []
        for alert_coro in alerts_tasks:
            alert_result = alert_coro.result()
            recorded_alerts.append(alert_result)
            if alert_result and alert_result.trigger_category == "critical":
                await alert_queue.put(f"New alert detected: {alert.timestamp}")

        logger.debug(f"Recorded alerts: {recorded_alerts}")
        return recorded_alerts

    def _extract_request_message(self, request: str) -> Optional[dict]:
        """Extract the user message from the FIM request"""
        try:
            parsed_request = json.loads(request)
        except Exception as e:
            logger.exception(f"Failed to extract request message: {request}", error=str(e))
            return None

        messages = [message for message in parsed_request["messages"] if message["role"] == "user"]
        if len(messages) != 1:
            logger.warning(f"Expected one user message, found {len(messages)}.")
            return None

        content_message = messages[0].get("content")
        return content_message

    def _create_hash_key(self, message: str, provider: str) -> str:
        """Creates a hash key from the message and includes the provider"""
        # Try to extract the path from the FIM message. The path is in FIM request in these formats:
        # folder/testing_file.py
        # Path: file3.py
        pattern = r"^#.*?\b([a-zA-Z0-9_\-\/]+\.\w+)\b"
        matches = re.findall(pattern, message, re.MULTILINE)
        # If no path is found, hash the entire prompt message.
        if not matches:
            logger.warning("No path found in messages. Creating hash cache from message.")
            message_to_hash = f"{message}-{provider}"
        else:
            # Copilot puts the path at the top of the file. Continue providers contain
            # several paths, the one in which the fim is triggered is the last one.
            if provider == "copilot":
                filepath = matches[0]
            else:
                filepath = matches[-1]
            message_to_hash = f"{filepath}-{provider}"

        logger.debug(f"Message to hash: {message_to_hash}")
        hashed_content = hashlib.sha256(message_to_hash.encode("utf-8")).hexdigest()
        logger.debug(f"Hashed contnet: {hashed_content}")
        return hashed_content

    def _should_record_context(self, context: Optional[PipelineContext]) -> bool:
        """Check if the context should be recorded in DB"""
        if context is None or context.metadata.get("stored_in_db", False):
            return False

        if not context.input_request:
            logger.warning("No input request found. Skipping recording context.")
            return False

        # If it's not a FIM prompt, we don't need to check anything else.
        if context.input_request.type != "fim":
            return True

        # Couldn't process the user message. Skip creating a mapping entry.
        message = self._extract_request_message(context.input_request.request)
        if message is None:
            logger.warning(f"Couldn't read FIM message: {message}. Will not record to DB.")
            return False

        hash_key = self._create_hash_key(message, context.input_request.provider)
        old_timestamp = fim_entries.get(hash_key, None)
        if old_timestamp is None:
            fim_entries[hash_key] = context.input_request.timestamp
            return True

        elapsed_seconds = (context.input_request.timestamp - old_timestamp).total_seconds()
        if elapsed_seconds < Config.get_config().max_fim_hash_lifetime:
            logger.info(
                f"Skipping DB context recording. "
                f"Elapsed time since last FIM cache: {timedelta(seconds=elapsed_seconds)}."
            )
            return False

    async def record_context(self, context: Optional[PipelineContext]) -> None:
        if not self._should_record_context(context):
            return
        await self.record_request(context.input_request)
        await self.record_outputs(context.output_responses)
        await self.record_alerts(context.alerts_raised)
        context.metadata["stored_in_db"] = True
        logger.info(
            f"Recorded context in DB. Output chunks: {len(context.output_responses)}. "
            f"Alerts: {len(context.alerts_raised)}."
        )


class DbReader(DbCodeGate):

    def __init__(self, sqlite_path: Optional[str] = None):
        super().__init__(sqlite_path)

    async def get_prompts_with_output(self) -> List[GetPromptWithOutputsRow]:
        conn = await self._async_db_engine.connect()
        querier = AsyncQuerier(conn)
        prompts = [prompt async for prompt in querier.get_prompt_with_outputs()]
        await conn.close()
        return prompts

    async def get_alerts_with_prompt_and_output(self) -> List[GetAlertsWithPromptAndOutputRow]:
        conn = await self._async_db_engine.connect()
        querier = AsyncQuerier(conn)
        prompts = [prompt async for prompt in querier.get_alerts_with_prompt_and_output()]
        await conn.close()
        return prompts


def init_db_sync(db_path: Optional[str] = None):
    """DB will be initialized in the constructor in case it doesn't exist."""
    db = DbRecorder(db_path)
    asyncio.run(db.init_db())


if __name__ == "__main__":
    init_db_sync()
